# -*- coding: utf-8 -*-
"""ml-terapan-1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KOwFIFwSja4p19wx04KyLiGLSlk8oI38

# **Import semua library**
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import LabelEncoder, StandardScaler

from sklearn.ensemble import RandomForestClassifier
import xgboost as xgb
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.model_selection import GridSearchCV, cross_val_score

"""# **Gathering Data**"""

df = pd.read_csv('diagnosed_cbc_data_v4.csv')

df.head()

"""# **Exploratory Data Analysis**

Mengecek informasi data
"""

df.info()

df.shape

"""Tidak terdapat missing value dalam dataset"""

df.isna().sum()

"""Terdapat juga data duplikat sebanyak 49 data"""

df.duplicated().sum()

df[df.duplicated()]

"""Karena jumlah duplikat sedikit dibandingkan total data yang cukup besar, maka data duplikat saya hapus."""

df = df.drop_duplicates()

df.duplicated().sum()

df.describe()

"""Distribusi total kategori diagnosis"""

categorical_col = 'Diagnosis'

plt.figure(figsize=(12, 6))
ax = sns.countplot(x=categorical_col, data=df)
for p in ax.patches:
    ax.text(p.get_x() + p.get_width()/2, p.get_height() + 5, int(p.get_height()), ha='center')
plt.title('Distribusi Kategori Diagnosis')
plt.xlabel('Diagnosis')
plt.ylabel('Jumlah')
plt.xticks(rotation=60, ha='right')
plt.tight_layout()
plt.show()

"""Distribusi semua fitur numerik"""

numerical_cols = df.select_dtypes(include='float64').columns

df[numerical_cols].hist(bins=15, figsize=(8, 10), layout=(5, 3))
plt.suptitle('Distribusi Histogram Fitur Numerik', y=1.02)
plt.tight_layout()
plt.show()

"""Pengecekan oulier

Outlier tidak dihapus karena dapat menjadi indikator penting dalam klasifikasi tipe anemia. Dengan 1.281 entri, pengaruhnya terhadap distribusi data minim. Selain itu, algoritma seperti Random Forest dan XGBoost cukup robust terhadap outlier.
"""

plt.figure(figsize=(16, 10))
for i, col in enumerate(numerical_cols, 1):
    plt.subplot(4, 4, i)
    sns.boxplot(y=df[col])
    plt.title(col)
plt.tight_layout()
plt.show()

sns.pairplot(df[numerical_cols])
plt.suptitle('Pair Plot of Selected Numerical Features')
plt.show()

correlation_matrix = df[numerical_cols].corr()
plt.figure(figsize=(12, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Korelasi Antar Fitur Numerik')
plt.show()

"""## **Data preparation**

Encoding fitur kategori yaitu diagnosis
"""

le = LabelEncoder()
df[categorical_col] = le.fit_transform(df[categorical_col])

df.head()

"""Normalisasi semua fitur numerik"""

scaler = StandardScaler()
df[numerical_cols] = scaler.fit_transform(df[numerical_cols])

df.head()

"""Pemisahan fitur dan target, serta train test split"""

X = df.drop('Diagnosis', axis=1)
y = df['Diagnosis']

# Split data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print(f'Jumlah data training (X_train): {X_train.shape}')
print(f'Jumlah data testing (X_test): {X_test.shape}')
print(f'Jumlah target training (y_train): {y_train.shape}')
print(f'Jumlah target testing (y_test): {y_test.shape}')

"""# **Modeling dan Hasil Evaluasi**

### **Random Forest**
"""

model_rf = RandomForestClassifier(
    n_estimators=100,
    max_depth=None,
    min_samples_split=2,
    random_state=None
)
model_rf.fit(X_train, y_train)

y_pred_rf = model_rf.predict(X_test)

accuracy_rf = accuracy_score(y_test, y_pred_rf)
report_rf = classification_report(y_test, y_pred_rf, zero_division=0)
cm_rf = confusion_matrix(y_test, y_pred_rf)

print(f"Random Forest Accuracy: {accuracy_rf}")
print("Random Forest Classification Report:")
print(report_rf)

plt.figure(figsize=(6, 4))
sns.heatmap(cm_rf, annot=True, fmt='d', cmap='Blues', cbar=False,
            xticklabels=model_rf.classes_, yticklabels=model_rf.classes_)
plt.title('Random Forest Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

"""### **XGBoost**"""

model_xgb = xgb.XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')
model_xgb.fit(X_train, y_train)

y_pred_xgb = model_xgb.predict(X_test)

accuracy_xgb = accuracy_score(y_test, y_pred_xgb)
report_xgb = classification_report(y_test, y_pred_xgb, zero_division=0)
cm_xgb = confusion_matrix(y_test, y_pred_xgb)

print(f"\nXGBoost Accuracy: {accuracy_xgb}")
print("XGBoost Classification Report:")
print(report_xgb)

plt.figure(figsize=(6, 4))
sns.heatmap(cm_xgb, annot=True, fmt='d', cmap='Greens', cbar=False,
            xticklabels=model_xgb.classes_, yticklabels=model_xgb.classes_)
plt.title('XGBoost Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

"""### **SVM**"""

model_svm = SVC(
    C=1.0,
    kernel='rbf',
    gamma='scale',
    random_state=None
)
model_svm.fit(X_train, y_train)

y_pred_svm = model_svm.predict(X_test)

accuracy_svm = accuracy_score(y_test, y_pred_svm)
report_svm = classification_report(y_test, y_pred_svm, zero_division=0)
cm_svm = confusion_matrix(y_test, y_pred_svm)

print(f"\nSVM Accuracy: {accuracy_svm}")
print("SVM Classification Report:\n", report_svm)

plt.figure(figsize=(6, 4))
sns.heatmap(cm_svm, annot=True, fmt='d', cmap='Reds', cbar=False,
            xticklabels=model_svm.classes_, yticklabels=model_svm.classes_)
plt.title('SVM Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

data = {
    'accuracy': [accuracy_rf, accuracy_xgb, accuracy_svm]
}

df_accuracy = pd.DataFrame(data, index=['Random Forest', 'XGBoost', 'SVM'])
df_accuracy

"""Sebelum tuning model random forest dan XGBoost memberikan hasil yang sama-sama sangat baik

# **Hyperparameter Tuning**
"""

# Setup hyperparameter grid dan model
param_grid = {
    'RF': {'n_estimators': [100, 200], 'max_depth': [10, 20], 'min_samples_split': [2, 5]},
    'SVM': {'C': [0.1, 1, 10], 'kernel': ['linear', 'rbf', 'poly'], 'gamma': ['scale', 'auto']},
    'XGB': {'n_estimators': [100, 200], 'max_depth': [3, 6], 'learning_rate': [0.01, 0.1]}
}

estimators = {
    'RF': RandomForestClassifier(),
    'SVM': SVC(),
    'XGB': xgb.XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')
}

# Simpan output terbaik
hasil = {}

for nama, clf in estimators.items():
    gs = GridSearchCV(clf, param_grid[nama], cv=5, scoring='accuracy', n_jobs=-1)
    gs.fit(X_train, y_train)

    best = gs.best_estimator_
    acc_cv = cross_val_score(best, X_train, y_train, cv=5).mean()
    acc_test = best.score(X_test, y_test)

    hasil[nama] = {
        'Param Terbaik': gs.best_params_,
        'Akurasi Cross Validation': round(acc_cv, 4),
        'Akurasi Test': round(acc_test, 4),
        'Evaluasi': "Overfit" if acc_cv > acc_test + 0.02 else "Underfit" if acc_test > acc_cv + 0.02 else "Fit"
    }

pd.set_option('display.max_colwidth', None)

df_hasil = pd.DataFrame.from_dict(hasil, orient='index').reset_index().rename(columns={'index': 'Model'})
df_hasil

"""Setelah dilakukan tuning hyperparameter, Random Forest dan XGBoost tetap menunjukkan performa yang tinggi dan stabil, meskipun akurasi test keduanya sedikit menurun dibandingkan sebelum tuning. Keduanya berada dalam kategori fit berdasarkan evaluasi hasil cross-validation dan data test. Sementara itu, SVM mengalami peningkatan akurasi yang signifikan setelah tuning, namun performanya masih belum menyamai dua model lainnya dan dikategorikan underfit."""

model_alias = {'RF': 'Random Forest', 'XGB': 'XGBoost', 'SVM': 'SVM'}
models = list(model_alias.keys())
accuracy_before = [accuracy_rf, accuracy_xgb, accuracy_svm]
accuracy_after = [df_hasil.loc[df_hasil['Model'] == m, 'Akurasi Test'].values[0] for m in models]

# Grafik
x = range(len(models))
width = 0.35

fig, ax = plt.subplots(figsize=(10, 6))
bars1 = ax.bar([i - width/2 for i in x], accuracy_before, width, label='Sebelum Tuning')
bars2 = ax.bar([i + width/2 for i in x], accuracy_after, width, label='Setelah Tuning')

ax.set_ylabel('Akurasi')
ax.set_title('Akurasi Model Sebelum dan Setelah Tuning')
ax.set_xticks(x)
ax.set_xticklabels([model_alias[m] for m in models])
ax.legend()

for bars in [bars1, bars2]:
    for bar in bars:
        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.005,
                f'{bar.get_height():.4f}', ha='center', va='bottom')

plt.tight_layout()
plt.show()